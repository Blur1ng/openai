
================================================================================
ФАЙЛ: Dockerfile
================================================================================

# Базовый образ Python
FROM python:3.10

# Установить рабочую директорию в контейнере
WORKDIR /app

# Установить переменные окружения для Python
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# Скопировать файл с зависимостями в контейнер
COPY requirements.txt /app/

# Установить зависимости
RUN pip install --no-cache-dir -r requirements.txt

# Скопировать код приложения в контейнер
COPY . /app/

# Указать команду для запуска приложения (замените на вашу команду)
CMD ["make", "run"]

================================================================================
ФАЙЛ: LICENSE
================================================================================

Copyright © 2024 Кирилл Хорьков

Настоящая лицензия (далее – «Лицензия») определяет условия использования программного обеспечения, созданного Кириллом Хорьковым (далее – «Владелец»), и всех сопутствующих материалов.

1. Определения

1.1. Программное обеспечение: Любые объектные и исходные коды, библиотеки, скрипты, исполняемые файлы, а также любая документация, руководства, руководства пользователя, изображения, видеоматериалы, аудиозаписи, иные материалы и/или ресурсы, предоставляемые Владельцем или создаваемые на их основе.

1.2. Пользователь: Любое лицо, физическое или юридическое, которое получила доступ к Программному обеспечению и использует его на условиях настоящей Лицензии.

1.3. Коммерческое использование: Любое использование Программного обеспечения, прямо или косвенно направленное на извлечение прибыли, включая, но не ограничиваясь: внедрение в платные проекты, продажу, sublicензионную передачу, размещение в составе коммерческих продуктов, оказание услуг с использованием Программного обеспечения.

1.4. Личное и некоммерческое использование: Использование Программного обеспечения, не направленное на извлечение прибыли, осуществляемое физическим лицом для личных целей, при котором Программное обеспечение не распространяется, не копируется и не модифицируется без соответствующих разрешений.

2. Право собственности и авторские права

2.1. Все исключительные права на Программное обеспечение, включая авторские права, товарные знаки и иные объекты интеллектуальной собственности, принадлежат Владельцу.

2.2. Программное обеспечение защищено нормами международного и национального законодательства об авторском праве и смежных правах, а также другими применимыми законами и договорами.

2.3. Лицензия не передает Пользователю каких-либо прав собственности или иных прав на Программное обеспечение, кроме прямо оговоренных в настоящем документе.

3. Предоставляемая Лицензия

3.1. Владелец предоставляет Пользователю ограниченную, непередаваемую, неисключительную лицензию на использование Программного обеспечения для личных, некоммерческих целей при условии соблюдения всех положений настоящей Лицензии.

3.2. Любое иное использование, не предусмотренное явным образом настоящей Лицензией, требует предварительного письменного разрешения Владельца.

4. Запрещенные действия

4.1. Копирование и распространение: Пользователь не вправе копировать, распространять, публиковать, передавать третьим лицам или иным образом делать доступным Программное обеспечение или его части без явного письменного согласия Владельца.

4.2. Изменение: Любое внесение изменений в исходный код, объектный код или иную часть Программного обеспечения, включая модификации, адаптации, переводы, декомпиляцию, реверс-инжиниринг и иные попытки извлечения исходного кода, без письменного разрешения Владельца запрещено.

4.3. Коммерческое использование: Использование Программного обеспечения для получения прямой или косвенной прибыли без заключения соответствующего лицензионного соглашения с Владельцем строго запрещено.

5. Условия использования

5.1. Личное и некоммерческое использование: Пользователь может использовать Программное обеспечение исключительно для собственных нужд, не связанных с предпринимательской, коммерческой или иной доходной деятельностью.

5.2. Отсутствие права передачи: Пользователь не вправе передавать третьим лицам Программное обеспечение или права на его использование, за исключением случаев, когда такое право явно предоставлено Владельцем в письменной форме.

5.3. Неизменность уведомлений о правах: Пользователь не вправе удалять, изменять или скрывать какие-либо уведомления об авторских правах, товарных знаках или другие указания на правообладание, содержащиеся в Программном обеспечении.

6. Отказ от гарантий

6.1. Предоставление «как есть»: Программное обеспечение предоставляется «как есть» («as is») без каких-либо гарантий, явных или подразумеваемых, включая, но не ограничиваясь гарантиями товарной пригодности, пригодности для определенной цели, отсутствия нарушений прав третьих лиц и т.п.

6.2. Риск использования: Пользователь самостоятельно и полностью несет риск, связанный с использованием Программного обеспечения, включая риск, связанный с его производительностью, надежностью, безопасностью, совместимостью и соответствием ожиданиям Пользователя.

7. Ограничение ответственности

7.1. Ни при каких обстоятельствах Владелец не несет ответственности за прямой, косвенный, случайный, особый, штрафной или любой иной ущерб (включая, помимо прочего, упущенную выгоду, потерю данных, прерывание деятельности, потерю деловой репутации или другие имущественные и неимущественные потери), возникающий в связи с использованием или невозможностью использования Программного обеспечения, даже если Владелец был уведомлен о возможности такого ущерба.

7.2. Если применимое законодательство не допускает исключение или ограничение определенных гарантий или ответственности, перечисленные выше ограничения и исключения будут действовать в максимально допустимой степени, предусмотренной таким законодательством.

8. Изменения в Лицензии

8.1. Владелец оставляет за собой право вносить изменения, дополнения или обновления в настоящее Лицензионное соглашение в одностороннем порядке в любое время.

8.2. О таких изменениях будет сообщено Пользователям через разумно доступные каналы (например, обновление файла LICENSE в репозитории проекта или оповещение на официальном сайте).

8.3. Дальнейшее использование Программного обеспечения после внесения изменений означает согласие Пользователя с новыми условиями.

9. Применимое право и разрешение споров

9.1. Настоящая Лицензия и все возникающие из нее или связанные с ней взаимоотношения регулируются применимым законодательством страны, определенной Владельцем (если не указано иное, по умолчанию применяется законодательство Российской Федерации).

9.2. Все споры, разногласия и требования, возникающие из или в связи с настоящей Лицензией, подлежат разрешению путем переговоров. В случае недостижения соглашения спор передается на рассмотрение компетентного суда, определенного в соответствии с применимым законодательством.

10. Раздельность положений

10.1. Если какое-либо из положений настоящей Лицензии будет признано недействительным, незаконным или не подлежащим принудительному исполнению, это не повлияет на действительность или применимость остальных положений.

================================================================================
ФАЙЛ: Makefile
================================================================================

ifneq ($(wildcard .env.example),)
	ENV_FILE = .env.example
endif
ifneq ($(wildcard .env.example),)
	ifeq ($(COMPOSE_PROJECT_NAME),)
		include .env.example
	endif
endif
ifneq ($(wildcard .env),)
	ENV_FILE = .env
endif
ifneq ($(wildcard .env),)
	ifeq ($(COMPOSE_PROJECT_NAME),)
		include .env
	endif
endif

export


.SILENT:
.PHONY: help
help: ## Display this help screen
	awk 'BEGIN {FS = ':.*##'; printf 'Usage:\n  make \033[36m<target>\033[0m\n'} /^[a-zA-Z_-]+:.*?##/ { printf '  \033[36m%-18s\033[0m %s\n', $$1, $$2 } /^##@/ { printf '\n\033[1m%s\033[0m\n', substr($$0, 5) }' $(MAKEFILE_LIST)


include make/run.Makefile
include make/lint.Makefile
include make/test.Makefile
include make/migration.Makefile
include make/compose.Makefile
include make/docker.Makefile

================================================================================
ФАЙЛ: README.md
================================================================================

# Linux Code Pipeline

Ниже представлена схема полного цикла обработки исходного кода в Linux с использованием LLVM, от исходных файлов до выполнения на оборудовании.
```mermaid
flowchart TD
    A["Source Code (C, C++, Rust, etc.)"]
    %% Исходные файлы с расширениями .c, .cpp, .rs и т.д.
    A --> B["Frontend"]
    %% Лексический, синтаксический и семантический анализ
    B --> C["AST"]
    %% Абстрактное синтаксическое дерево
    C --> D["LLVM IR"]
    %% Преобразование AST в промежуточное представление SSA
    D --> E["IR Optimizations"]
    %% Инлайнинг, векторизация, удаление мёртвого кода и др.
    E --> F["Code Generation"]
    %% Понижение IR до инструкций целевой архитектуры
    F --> G["Assembly (.s)"]
    %% Текст ассемблера для конкретного CPU
    G --> H["Assembler"]
    %% Преобразование ассемблера в объектный код
    H --> I["Object File (.o)"]
    %% Содержит машинные инструкции и таблицы символов
    I --> J["Linker"]
    %% Объединение нескольких .o и библиотек в один ELF
    J --> K["Executable (ELF)"]
    %% Готовый исполняемый файл
    K --> L["Loader"]
    %% Загрузка программы в память
    L --> M["Dynamic Linking"]
    %% Подключение динамических библиотек (.so)
    M --> N["Process"]
    %% Запущенный процесс в пространстве пользователя
    N --> O["System Calls"]
    %% Вызовы ядра для доступа к ресурсам (файлы, сеть и т.д.)
    O --> P["Kernel Space"]
    %% Ядро выполняет операции с оборудованием
    P --> Q["Hardware"]
    %% Фактические действия на уровне железа
```

## Подробное описание этапов

1. **Source Code (Исходный код)**

   * Текст программы, написанный на высокоуровневом языке (C, C++, Rust и др.).
   * Хранится в файлах с расширениями `.c`, `.cpp`, `.rs`.

2. **Frontend**

   * Лексический анализ (токенизация), синтаксический анализ (парсинг) и семантический анализ.
   * Проверяет корректность синтаксиса, типов, областей видимости.

3. **AST (Abstract Syntax Tree)**

   * Абстрактное синтаксическое дерево, отражающее структуру программы.

4. **LLVM IR**

   * Промежуточное представление в формате SSA, независимое от языка и архитектуры.

5. **IR Optimizations**

   * Оптимизации: инлайнинг, удаление мёртвого кода, константное свёртывание, векторизация.

6. **Code Generation**

   * Трансформация IR в инструкции целевой архитектуры с учётом её особенностей.

7. **Assembly (.s)**

   * Файл с ассемблерными инструкциями для конкретного CPU.

8. **Assembler**

   * Преобразует ассемблерный код в объектный файл (`.o`).

9. **Object File (.o)**

   * Двоичный модуль с машинным кодом, таблицами символов, отладочной информацией.

10. **Linker**

    * Объединяет объектные файлы и библиотеки (.a, .so) в единый ELF.

11. **Executable (ELF)**

    * Готовый к запуску исполняемый файл формата ELF.

12. **Loader**

    * Загрузка ELF в память ядром Linux (`execve`).

13. **Dynamic Linking**

    * Подключение динамических библиотек (.so) во время выполнения.

14. **Process**

    * Запущенный экземпляр программы в пространстве пользователя.

15. **System Calls**

    * Привилегированные вызовы из процесса в ядро для доступа к ресурсам.

16. **Kernel Space**

    * Режим выполнения ядра, обрабатывающего системные вызовы и управляет оборудованием.

17. **Hardware (Аппаратное обеспечение)**

    * Фактическое выполнение машинного кода на уровне процессора, памяти и устройств.





🔹 Системные поля
•	id BIGSERIAL PRIMARY KEY
Уникальный идентификатор записи.
BIGSERIAL автоматически увеличивается при вставке новой строки → удобно, не нужно вручную генерировать ID.
•	created_at TIMESTAMP NOT NULL DEFAULT NOW()
Дата и время создания записи.
Нужно, чтобы видеть, когда строка появилась в таблице.
•	updated_at TIMESTAMP NOT NULL DEFAULT NOW()
Дата и время последнего изменения записи.
Обычно обновляется триггером при UPDATE, чтобы всегда знать, когда в последний раз меняли данные.
•	is_deleted BOOLEAN NOT NULL DEFAULT FALSE
Флаг “мягкого удаления” (soft delete).
Вместо того чтобы реально удалять строки из базы (и терять историю), мы ставим TRUE, если запись считается удалённой.

⸻

🔹 Предметные поля переписи
•	census_date DATE NOT NULL
Дата проведения переписи для этой записи.
Например, 2025-09-24.
•	full_name CHARACTER VARYING(255) NOT NULL
ФИО или наименование объекта (например, человек, домохозяйство, организация).
VARCHAR(255) даёт гибкость по длине строки.
•	population_count NUMERIC(12,2)
Количество учтённых людей.
NUMERIC(12,2) позволяет хранить большие числа и даже десятичные дроби (если, например, считать не только людей, а “человеко-единицы” с весом).
•	is_resident BOOLEAN NOT NULL DEFAULT TRUE
Признак, является ли человек/объект резидентом (постоянно проживает на территории).
TRUE → резидент, FALSE → временно или нерезидент.
•	stay_interval INTERVAL
Интервал пребывания (например, 2 years 3 months, 5 days).
Может показывать, сколько времени человек проживает на территории или как долго длится перепись для этой записи.

⸻

👉 Таким образом:
•	первые 4 поля — служебные (удобство администрирования и версионирования данных),
•	остальные — предметные, описывают сущность переписи.

⸻


================================================================================
ФАЙЛ: alembic.ini
================================================================================

# A generic, single database configuration.

[alembic]
# path to migration scripts
script_location = migrations

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python-dateutil library that can be
# installed by adding `alembic[tz]` to the pip requirements
# string value is passed to dateutil.tz.gettz()
# leave blank for localtime
# timezone =

# max length of characters to apply to the
# 'slug' field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to migrations/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by 'version_path_separator' below.
# version_locations = %(here)s/bar:%(here)s/bat:migrations/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is 'os', which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
version_path_separator = os  # Use os.pathsep. Default configuration used for new projects.

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

sqlalchemy.url = driver://user:pass@localhost/dbname


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using 'black' - use the console_scripts runner, against the 'black' entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S

================================================================================
ФАЙЛ: bot.py
================================================================================

import logging
import asyncio
import io
from uuid import uuid4

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
import uvicorn

from aiogram import Bot, Dispatcher, F, types, Router
from aiogram.filters import Command, StateFilter
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.context import FSMContext
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.types import InlineKeyboardButton, InlineKeyboardMarkup

import aiohttp

API_TOKEN = "7903004765:AAEapDcgiLPw8JDt6OxDssIyRzUKsXxH8w8"

# Ваш эндпоинт, куда загружаем файл (multipart/form-data)
YOUR_API_ENDPOINT = "https://api.student-space.ru/api/v1/docs/upload-pdf"

logging.basicConfig(level=logging.INFO)

# ------------------------------------------------------------------------------------
# Инициализация бота и диспетчера

bot = Bot(token=API_TOKEN)
storage = MemoryStorage()
dp = Dispatcher(storage=storage)
router = Router()

dp.include_router(router)

# Глобальный набор для блокировки пользователей
locked_users = set()


# ------------------------------------------------------------------------------------
# Состояния (FSM)

class Form(StatesGroup):
    waiting_for_action = State()
    waiting_for_pdf = State()


# ------------------------------------------------------------------------------------
# Создаём FastAPI-приложение
app = FastAPI()


# ------------------------------------------------------------------------------------
# 1. Хендлер /start

@router.message(Command("start"))
async def cmd_start_handler(message: types.Message, state: FSMContext):
    """Показываем меню выбора действия и переходим в состояние waiting_for_action."""
    await state.clear()
    await state.set_state(Form.waiting_for_action)

    keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [
            InlineKeyboardButton(text="Пересказ", callback_data="retell"),
            InlineKeyboardButton(text="Лекции по вопросам", callback_data="test")
        ],
        [
            InlineKeyboardButton(text="Вопросы по лекциям", callback_data="questions"),
            InlineKeyboardButton(text="Перевод", callback_data="translate")
        ]
    ])
    await message.answer("Выберите действие:", reply_markup=keyboard)


# ------------------------------------------------------------------------------------
# 2. Обработчик нажатия кнопок (retell, lectures, questions, translate)
#    Срабатывает и в waiting_for_action, и в waiting_for_pdf.

@router.callback_query(
    F.data.in_({"retell", "test", "questions", "translate"}),
    StateFilter(Form.waiting_for_action, Form.waiting_for_pdf),
)
async def process_action_callback(callback_query: types.CallbackQuery, state: FSMContext):
    action = callback_query.data

    # Сохраняем действие в FSM
    await state.update_data(selected_action=action)

    # Собираем новую клавиатуру (галочки)
    retell_text = "✅ Пересказ" if action == "retell" else "Пересказ"
    lectures_text = "✅ Лекции по вопросам" if action == "test" else "Лекции по вопросам"
    questions_text = "✅ Вопросы по лекциям" if action == "questions" else "Вопросы по лекциям"
    translate_text = "✅ Перевод" if action == "translate" else "Перевод"

    new_keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [
            InlineKeyboardButton(text=retell_text, callback_data="retell"),
            InlineKeyboardButton(text=lectures_text, callback_data="test")
        ],
        [
            InlineKeyboardButton(text=questions_text, callback_data="questions"),
            InlineKeyboardButton(text=translate_text, callback_data="translate")
        ]
    ])

    current_state = await state.get_state()
    if current_state == Form.waiting_for_action:
        # Первый выбор действия: переводим пользователя в waiting_for_pdf
        text_for_user = "Вы выбрали действие. Теперь отправьте PDF-файл (до 5 МБ)."
        await state.set_state(Form.waiting_for_pdf)
    else:
        # Если пользователь УЖЕ в waiting_for_pdf и нажал другую кнопку —
        # просто меняем действие.
        text_for_user = "Вы сменили действие. Теперь отправьте PDF-файл (до 5 МБ)."

    # Обновляем сообщение
    await callback_query.message.edit_text(text=text_for_user, reply_markup=new_keyboard)
    await callback_query.answer()


# ------------------------------------------------------------------------------------
# 3. Пришёл документ в состоянии waiting_for_pdf

@router.message(F.document, Form.waiting_for_pdf)
async def process_pdf_handler(message: types.Message, state: FSMContext):
    user_id = message.from_user.id

    document = message.document
    # Проверяем PDF
    if document.mime_type != "application/pdf":
        await message.answer("Это не PDF-файл. Пожалуйста, пришлите PDF.")
        return

    # Ограничение размера 5 МБ
    if document.file_size > 5 * 1024 * 1024:
        await message.answer("Файл слишком большой. Максимальный размер — 5 МБ.")
        return

    # Достаем выбранное действие
    data = await state.get_data()
    selected_action = data.get("selected_action", "no_action")

    file_info = await bot.get_file(document.file_id)
    file_bytes = await bot.download_file(file_info.file_path)

    # Готовим multipart/form-data
    form_data = aiohttp.FormData()
    form_data.add_field(
        name="file",
        value=file_bytes,
        filename=f"{uuid4()}.pdf",
        content_type="application/pdf"
    )
    form_data.add_field(name="user_id", value=str(user_id))
    form_data.add_field(name="prompt_type", value=selected_action)
    # Если нужно, можно передать и действие
    # form_data.add_field(name="action", value=selected_action)

    headers = {
        "X-Admin-Header": "FsfY1VXAHrzTXUDZ57yiNrqXkRbF0"
    }

    async with aiohttp.ClientSession() as session:
        try:
            resp = await session.post(YOUR_API_ENDPOINT, headers=headers, data=form_data)
            if resp.status == 200:
                logging.info(f"Файл от user_id={user_id} успешно отправлен на API")
            else:
                logging.error(f"Ошибка загрузки файла: статус {resp.status} (user_id={user_id})")
        except Exception as e:
            logging.error(f"Исключение при отправке файла user_id={user_id}: {e}")

    await message.answer(
        "PDF получен и отправлен на сервер!\n"
        "Теперь подождите, пока файл будет обработан.\n"
        "После окончания обработки вы сможете отправить новый файл."
    )


# ------------------------------------------------------------------------------------
# 4. Если пользователь шлёт что-то не то (вместо PDF), когда мы ждём PDF

@router.message(StateFilter(Form.waiting_for_pdf))
async def process_non_pdf_handler(message: types.Message):
    await message.answer("Пожалуйста, пришлите PDF-файл (до 5 МБ).")


# ------------------------------------------------------------------------------------
# 5. Эндпойнт /external-webhook (от стороннего сервиса)
#    Здесь мы «разблокируем» пользователя и можем отправить файл обратно, если нужно.

@app.post("/external-webhook")
async def external_webhook(request: Request):
    data = await request.json()
    user_id = data.get("user_id")
    file_url = data.get("file_url")

    print(user_id)

    if not user_id:
        return JSONResponse({"ok": False, "error": "user_id is required"}, status_code=400)

    if file_url:
        # Отправляем документ напрямую (мы в асинхронном контексте, можно просто await)

        await bot.send_document(
            chat_id=user_id,
            document=file_url,
            caption="Вот ваш обработанный файл!"
        )

    return {"ok": True}


# ------------------------------------------------------------------------------------
# Запускаем и FastAPI, и бота вместе в одном event loop

async def run_fastapi():
    """Запуск Uvicorn-сервера (FastAPI) асинхронно."""
    config = uvicorn.Config(app, host="127.0.0.1", port=8100, log_level="info")
    server = uvicorn.Server(config)
    await server.serve()  # Блокирующе, но асинхронно


async def run_bot():
    """Запуск aiogram (long polling)."""
    await dp.start_polling(bot)


async def main():
    # Запускаем обе задачи параллельно в одном event loop
    bot_task = asyncio.create_task(run_bot())
    fastapi_task = asyncio.create_task(run_fastapi())
    await asyncio.gather(bot_task, fastapi_task)


if __name__ == "__main__":
    asyncio.run(main())

================================================================================
ФАЙЛ: cmd/app/main.py
================================================================================

import sys
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parents[2]
sys.path.append(str(BASE_DIR))

from internal.app import backend_app  # noqa: E402

app = backend_app()

================================================================================
ФАЙЛ: docker/docker-compose.yml
================================================================================

version: '3.7'

networks:
  clickhouse-net:
    driver: bridge
services:
  zookeeper:
    image: zookeeper:3.9.3
    container_name: zh-zookeeper
    restart: unless-stopped
    networks:
      - clickhouse-net
    ports:
      - "127.0.0.1:2185:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  clickhouse1:
    image: clickhouse/clickhouse-server:latest
    container_name: ch1
    depends_on:
      - zookeeper
    networks:
      - clickhouse-net
    volumes:
      - ./configs:/etc/clickhouse-server/
      - ./init-clickhouse.sh:/docker-entrypoint-initdb.d/init-clickhouse.sh:ro
      - /var/clickhouse_data/ch1:/var/lib/clickhouse
    ports:
      - '127.0.0.1:${MILVUS_GRPC_INTERNAL_PORT}:${MILVUS_GRPC_PORT}'
      - '127.0.0.1:${MILVUS_API_INTERNAL_PORT}:${MILVUS_API_PORT}'
    depends_on:
      - etcd
      - minio
    networks:
      bot_network:
        ipv4_address: ${MILVUS_DOCKER_IP}


  postgres:
    <<: *default
    container_name: postgres
    image: postgres:latest
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    ports:
      - '127.0.0.1:${DB_PORT}:${DB_PORT_INTERNAL}'
    volumes:
      - ./configs:/etc/clickhouse-server/
      - ./init-clickhouse.sh:/docker-entrypoint-initdb.d/init-clickhouse.sh:ro
      - /var/clickhouse_data/ch2:/var/lib/clickhouse
    ports:
      - "127.0.0.1:9992:9000"
      - "127.0.0.1:8125:8123"

  clickhouse3:
    image: clickhouse/clickhouse-server:latest
    container_name: ch3
    depends_on:
      - zookeeper
    networks:
      - clickhouse-net
    volumes:
      - ./configs:/etc/clickhouse-server/
      - ./init-clickhouse.sh:/docker-entrypoint-initdb.d/init-clickhouse.sh:ro
      - /var/clickhouse_data/ch3:/var/lib/clickhouse
    ports:
      - "127.0.0.1:9995:9000"
      - "127.0.0.1:8126:8123"

================================================================================
ФАЙЛ: internal/__init__.py
================================================================================


================================================================================
ФАЙЛ: internal/app/__init__.py
================================================================================

from .app import create_app as backend_app

================================================================================
ФАЙЛ: internal/app/app.py
================================================================================

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy.exc import DBAPIError, NoResultFound

from internal.config import settings
from internal.config.modules import database
from internal.controller.http.router import api_router
from internal.usecase.utils import (
    database_error_handler,
    database_not_found_handler,
    http_exception_handler,
)
from internal.usecase.utils.responses import DynamicResponse


def create_app() -> FastAPI:
    app = FastAPI(
        title=settings.NAME,
        description=settings.DESCRIPTION,
        version=settings.VERSION,
        openapi_url='{0}/openapi.json'.format(settings.DOCS),
        swagger_ui_parameters=settings.SWAGGER_UI_PARAMETERS,
    )

    if settings.APP_CORS_ORIGINS:
        app.add_middleware(
            CORSMiddleware,
            allow_origins=[
                str(origin)
                for origin in settings.APP_CORS_ORIGINS
            ],
            allow_credentials=True,
            allow_methods=['*'],
            allow_headers=['*'],
        )

    @app.middleware("http")
    async def check_header(request: Request, call_next):
        """
        Function to create and configure a FastAPI application instance.

        This function sets up the FastAPI application and adds necessary configurations, such as the
        middleware used for validating specific HTTP headers. The middleware checks for the existence
        and correctness of a required header key and value in all incoming requests. If the header is
        missing or invalid, the request is rejected with a 403 status code and an appropriate error
        message is returned.

        Returns:
            An instance of the FastAPI application with the defined middleware applied.

        Raises:
            HTTPException: When the required header key is not present or contains an invalid value.
        """
        if settings.DEBUG:
            return await call_next(request)
        required_header_key = "X-Admin-Header"
        required_header_value = settings.ADMIN_KEY

        if request.headers.get(required_header_key) != required_header_value:
            return DynamicResponse.create(
                status_code=403,
                description='Invalid or missing header',
                detail='Forbidden',
            )

        response = await call_next(request)
        return response

    app.include_router(api_router, prefix=settings.API)
    app.dependency_overrides.setdefault(*database.override_session)

    app.add_exception_handler(DBAPIError, database_error_handler)
    app.add_exception_handler(HTTPException, http_exception_handler)
    app.add_exception_handler(NoResultFound, database_not_found_handler)

    return app

================================================================================
ФАЙЛ: internal/config/__init__.py
================================================================================

from .settings import settings
from .modules.milvus import get_milvus_client
from .modules.gpt import get_gpt_client
from .modules.minio import get_minio_client
from .modules.database import get_database_client, override_session

================================================================================
ФАЙЛ: internal/config/modules/__init__.py
================================================================================


================================================================================
ФАЙЛ: internal/config/modules/database.py
================================================================================

from typing import Any, AsyncContextManager, AsyncGenerator, Callable

from sqlalchemy import orm
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine

from internal.config import settings
from internal.usecase.utils import get_session

AsyncSessionGenerator = AsyncGenerator[AsyncSession, None]


def async_session(
        url: str, *, wrap: Callable[..., Any] | None = None,  # noqa: WPS318
) -> Callable[..., AsyncSessionGenerator] | AsyncContextManager[Any]:
    engine = create_async_engine(
        url, pool_pre_ping=True, future=True,
    )
    factory = orm.sessionmaker(
        engine, class_=AsyncSession, autoflush=False, expire_on_commit=False,
    )

    async def get_session() -> AsyncSessionGenerator:  # noqa: WPS430, WPS442
        async with factory() as session:
            yield session

    return get_session if wrap is None else wrap(get_session)


override_session = get_session, async_session(settings.migrations_url)


def get_database_client():
    from internal.config import settings
    # Настраиваем создание сессии
    return async_session(settings.migrations_url)()

================================================================================
ФАЙЛ: internal/config/modules/gpt.py
================================================================================

# Фабрика для MinioClient
from internal.config import settings
from package.openai import ChatGPTClient, PromptManager
from pydantic import SecretStr

manager_prompt = PromptManager()
system_prompt = manager_prompt.get_prompt('test')

api_key = SecretStr(settings.OPENAI_TOKEN)
client = ChatGPTClient(
    api_key,
    model_name='gpt-4o-mini',
    embeddings_model_name='text-embedding-ada-002',
    system_prompt=system_prompt,
)


def get_gpt_client() -> ChatGPTClient:
    return client

================================================================================
ФАЙЛ: internal/config/modules/milvus.py
================================================================================

# Фабрика для MinioClient
from internal.config import settings
from package.milvus import MilvusClient
from package.milvus.main import MilvusClient

milvus_client = MilvusClient(host=settings.MILVUS_HOST, port=settings.MILVUS_PORT)


def get_milvus_client() -> MilvusClient:
    return milvus_client

================================================================================
ФАЙЛ: internal/config/modules/minio.py
================================================================================

# Фабрика для MinioClient
from internal.config import settings
from package.minio.main import MinioClient

minio_client = MinioClient(
    endpoint=f"{settings.MINIO_HOST}:{settings.MINIO_PORT}",
    access_key=settings.MINIO_ACCESS_KEY,
    secret_key=settings.MINIO_SECRET_KEY,
)


def get_minio_client() -> MinioClient:
    return minio_client

================================================================================
ФАЙЛ: internal/config/settings.py
================================================================================

from typing import ClassVar, Optional

from pydantic import Field, PostgresDsn, field_validator, RedisDsn
from pydantic_core.core_schema import ValidationInfo
from pydantic_settings import BaseSettings

buckets = {
    'pdf': 'pdf-bucket',
    'tmp': 'tmp'
}

MAX_FILE_SIZE = 5 * 1024 * 1024  # 5 MB


class Settings(BaseSettings):
    API: str = '/api'
    DOCS: str = '/docs'
    STARTUP: str = 'startup'
    SHUTDOWN: str = 'shutdown'
    COLLECTION_NAME: str = 'pdf_embeddings'

    NAME: str = 'Atlas Backend'
    VERSION: str = '0.1.0'
    DESCRIPTION: str = 'Atlas Backend'
    SWAGGER_UI_PARAMETERS: ClassVar[dict] = {'filter': True, 'displayRequestDuration': True}
    APP_CORS_ORIGINS: None = None
    ADMIN_KEY: str = Field('234df$13r', description='Key for get admin access.')
    DEBUG: bool = Field(False, description='Debug mode.')
    # Настройки базы данныхMutable default '[]' is not allowed. Use 'default_factory
    DB_HOST: str = Field(..., alias='DB_DOCKER_IP', description='Хост для подключения к базе данных.')
    DB_PORT: int = Field(..., description='Порт для подключения к базе данных.')
    DB_USER: str = Field(..., description='Имя пользователя для доступа к базе данных.')
    DB_PASSWORD: str = Field(..., description='Пароль для доступа к базе данных.')
    DB_NAME: str = Field(..., description='Имя базы данных.')
    DB_URI: Optional[PostgresDsn] = Field(None, description='URI для подключения к базе данных.')

    # Настройки Minio
    MINIO_HOST: str = Field('localhost', alias='MINIO_DOCKER_IP', description='Minio host for set connection.')
    MINIO_PORT: int = Field(5432, description='Default port for MinIO S3 server connection.')
    MINIO_ACCESS_KEY: str = Field(..., description='Minio access token.')
    MINIO_SECRET_KEY: str = Field(..., description='Minio secret token.')
    # Настройки OpenAI
    OPENAI_TOKEN: str = Field(..., description='OpenAI API Bearer token.')

    # Настройки Milvus
    MILVUS_HOST: str = Field('127.0.0.1', alias='MILVUS_DOCKER_IP', description='Milvus host for set connection.')
    MILVUS_PORT: int = Field(9091, alias='MILVUS_GRPC_PORT', description='Milvus port for set connection.')

    # Настройки Redis
    REDIS_HOST: str = Field('127.0.0.1', alias='REDIS_DOCKER_IP', description='Redis host for set connection.')
    REDIS_PORT: int = Field(..., description='Redis port for set connection.')
    REDIS_NAME: str = Field(..., description='Redis name for set connection.')

    # Настройки Celery
    CELERY_RESULT_BACKEND: RedisDsn | str | None = Field(None, description='Celery result backend URL.')
    CELERY_BROKER_URL: RedisDsn | str | None = Field(None, description='Celery broker URL.')

    TELEGRAM_WEBHOOK: str = Field('https://localhost', description='Telegram webhook for send data.')

    @field_validator('DB_URI', mode='before')
    @classmethod
    def assemble_db_connection(
            cls, value: str | None, values: ValidationInfo,  # noqa: WPS110
    ) -> str | PostgresDsn:
        if isinstance(value, str):
            return value

        return PostgresDsn.build(
            scheme='postgresql+asyncpg',
            username=values.data.get('DB_USER'),
            password=values.data.get('DB_PASSWORD'),
            host=values.data.get('DB_HOST'),
            port=values.data.get('DB_PORT'),
            path=str(values.data.get('DB_NAME')),
        )

    @field_validator('CELERY_RESULT_BACKEND', 'CELERY_BROKER_URL', mode='before')
    @classmethod
    def assemble_celery_connection(cls, value: str | None, values: ValidationInfo) -> str | RedisDsn:
        return RedisDsn.build(
            scheme='redis',
            host=values.data.get('REDIS_HOST'),
            port=values.data.get('REDIS_PORT'),
            path=str(values.data.get('REDIS_NAME')),
        )

    @property
    def migrations_url(self) -> str:
        # Преобразовать объект PostgresDsn в строку
        return str(self.DB_URI)

    class Config(object):
        env_file = '.env'
        env_file_encoding = 'utf-8'
        extra = 'ignore'


settings = Settings()

================================================================================
ФАЙЛ: internal/controller/http/router.py
================================================================================

from fastapi import APIRouter

from internal.controller.http import v1

api_router = APIRouter()
api_router.include_router(v1.router, prefix='/v1')

================================================================================
ФАЙЛ: internal/controller/http/v1/__init__.py
================================================================================

from fastapi import APIRouter, Depends
from .docs import router as docs_router

# router = APIRouter(dependencies=[Depends(dependencies.authorize_user)]) # noqa: E800

router = APIRouter()

router.include_router(docs_router, prefix='/docs')

================================================================================
ФАЙЛ: internal/controller/http/v1/docs.py
================================================================================

import uuid

from fastapi import APIRouter, UploadFile, File, Depends, Form

from internal.config.modules.minio import get_minio_client
from internal.config.settings import buckets, MAX_FILE_SIZE
from internal.dto.celery import TaskRunInfo
from internal.dto.docs import DocsCreate
from internal.service.docs import DocsService
from internal.usecase.utils.responses import HTTP_400_BAD_REQUEST, HTTP_200_OK_REQUEST, DynamicResponse
from package.minio.main import MinioClient
from package.celery.worker import process_document

# Создаем объект Router для маршрутов данного модуля
router = APIRouter()


@router.post(
    "/upload-pdf",
    summary="Загрузка и проверка PDF файла",
    responses={
        **HTTP_400_BAD_REQUEST.schema(
            status_code=400,
            description='Failed to upload file.',
            example={"detail": "Invalid MIME type. Expected application/pdf."}
        ),
        **HTTP_200_OK_REQUEST.schema(
            status_code=200,
            description='Success',
            example={"id": "1234567890", "filename": "example.pdf", "filesize": 1024}
        ),
    },
    tags=["PDF Upload"])
async def upload_pdf(
        user_id: str = Form(...),  # user_id приходит из формы
        prompt_type: str = Form(...),
        file: UploadFile = File(...),  #
        service: DocsService = Depends(DocsService),
        minio_client: MinioClient = Depends(get_minio_client),
):
    """
    Handles the upload of a PDF file, validates its MIME type and size, and stores
    the file in a specified bucket. The function initiates a background task for
    processing the uploaded document and returns a response with task details or
    an appropriate error message if validation or file upload fails.

    Args:
        user_id (str): The ID of the user uploading the file.
        file (UploadFile): An uploaded file object to be validated and processed.
        service (DocsService): A dependency injection providing access to the
            document service.
        minio_client (MinioClient): A dependency injection providing access to
            the MinIO client.
        prompt_type (str): The type of the prompt.

    Returns:
        DynamicResponse: A dynamic response indicating the result of the request.
        On success (200): Includes task information, target file name, and
            file size in task details.
        On failure (400): Includes details of the failure and corresponding
            messages such as MIME type verification or file size violations.
    """
    # Проверяем, что файл имеет расширение .pdf
    if file.content_type != "application/pdf":
        return DynamicResponse.create(status_code=400, description='Invalid MIME type. Expected application/pdf.')

    if file.size >= MAX_FILE_SIZE:
        return DynamicResponse.create(
            status_code=400,
            description='File size exceeds the limit.',
            detail=f'File size must be less than {MAX_FILE_SIZE / 1024} KB.',
        )

    bucket = buckets.get('tmp')
    object_name = f"{uuid.uuid4()}.pdf"
    s3_briefly = f"{bucket}/{object_name}"

    doc_data = DocsCreate(
        name=object_name,
        s3_briefly=s3_briefly,
    )
    try:
        await service.transaction_to_minio(
            minio_client=minio_client,
            dto=doc_data,
            bucket=bucket,
            file=file.file,
        )
        task = process_document.delay(object_name, bucket, user_id, prompt_type)
        task_info = TaskRunInfo(id=task.id, filename=object_name, filesize=file.size)
        return DynamicResponse.create(
            status_code=200,
            detail='Success',
            description='File successfully uploaded.',
            example=task_info.model_dump())
    except Exception as e:
        return DynamicResponse.create(status_code=400, detail="Bad Request", description=str(e))

================================================================================
ФАЙЛ: internal/dto/__init__.py
================================================================================


================================================================================
ФАЙЛ: internal/dto/celery.py
================================================================================

from pydantic import BaseModel


class TaskRunInfo(BaseModel):
    id: str
    filename: str
    filesize: float | int

================================================================================
ФАЙЛ: internal/dto/docs.py
================================================================================

from typing import Optional
from uuid import UUID
from pydantic import BaseModel


class DocsRead(BaseModel):
    id: UUID
    name: str
    s3_briefly: str

    class Config:
        from_attributes = True


class DocsCreate(BaseModel):
    """
    Represents a model for creating documentation files with a name and an S3 path.

    The class is designed to handle file metadata, including its name and S3
    storage path. It extends the BaseModel to provide model-related functionality
    and supports configuration settings to allow creating instances from
    class attributes.
    """
    name: str  # Имя файла
    s3_briefly: str  # Путь в хранилище S3

    class Config:
        from_attributes = True


class MilvusDocsCreate(BaseModel):
    """
    Represents a model for mapping Milvus IDs to document IDs.

    This class is designed to facilitate the integration and mapping between
    Milvus, a high-performance vector database, and document storage systems. Each
    instance of this class links a unique ID used in Milvus to a corresponding
    document ID. It ensures that both identifiers are consistently paired and can
    be utilized for further operations like querying or storage manipulation.

    Attributes:
        milvus_id (int): The unique identifier associated with an entry in the
            Milvus database.
        docs_id (int): The unique identifier associated with a document in the
            document storage system.

    Config:
        from_attributes (bool): Determines whether an instance of the class can
            be created from attributes directly. This is primarily used to enable
            flexibility in the data parsing and instantiation process.
    """
    milvus_id: int
    docs_id: int

    class Config:
        from_attributes = True


class MilvusDocsRead(BaseModel):
    """
    Represents a model for linking Milvus entries with document data.

    This class is used to represent a relationship between a Milvus entry and
    its corresponding document data. It contains references to the Milvus ID,
    a unique document identifier (UUID), and optionally, the associated
    document data. It is primarily intended for use as part of a database
    model or data-access-layer structure.

    Attributes:
        milvus_id: int
            The ID representing the entry in the Milvus system.
        docs_id: UUID
            The universally unique identifier (UUID) representing the
            associated document.
        docs: Optional[DocsRead]
            The optional data associated with the document referenced by
            the docs_id.

    Config:
        from_attributes: bool
            Enables mapping model attributes directly from an instance.
        arbitrary_types_allowed: bool
            Permits arbitrary types for properties in the model.
    """
    milvus_id: int
    docs_id: UUID  # UUID для внешнего ключа
    docs: Optional[DocsRead] = None  # Связанные данные из таблицы Docs

    class Config:
        from_attributes = True
        arbitrary_types_allowed = True

================================================================================
ФАЙЛ: internal/entity/__init__.py
================================================================================


================================================================================
ФАЙЛ: internal/entity/base.py
================================================================================

from re import sub

import sqlalchemy as sa
from sqlalchemy import MetaData
from sqlalchemy.dialects import postgresql as psql
from sqlalchemy.ext.declarative import as_declarative, declared_attr


@as_declarative()
class Base(object):

    __name__: str
    metadata: MetaData

    @classmethod
    @declared_attr
    def __tablename__(cls):
        return sub('(?<!^)(?=[A-Z])', '_', cls.__name__).lower()

    id = sa.Column(
        psql.UUID(as_uuid=True),
        server_default=sa.text('gen_random_uuid()'),
        primary_key=True,
        index=True,
    )

================================================================================
ФАЙЛ: internal/entity/docs.py
================================================================================

import sqlalchemy as sa
from sqlalchemy.dialects import postgresql as psql

from internal.entity.base import Base
from internal.entity.mixin import TimestampMixin
from sqlalchemy.orm import relationship


class Docs(TimestampMixin, Base):
    name = sa.Column(sa.String(255), nullable=True)
    checksum = sa.Column(sa.BigInteger, nullable=True)
    s3_briefly = sa.Column(sa.String(255), nullable=True)
    milvus_docs = relationship('MilvusDocs', back_populates='docs', uselist=True)


class MilvusDocs(TimestampMixin, Base):
    id = ...
    milvus_id = sa.Column(sa.BigInteger, primary_key=True)
    docs_id = sa.Column(psql.UUID(as_uuid=True), sa.ForeignKey('docs.id'), nullable=False)
    docs = relationship('Docs', back_populates='milvus_docs')

================================================================================
ФАЙЛ: internal/entity/mixin.py
================================================================================

import sqlalchemy as sa
from sqlalchemy.dialects import postgresql as psql
from sqlalchemy.orm import declarative_mixin


@declarative_mixin
class TimestampMixin(object):

    created_at = sa.Column(
        psql.TIMESTAMP(timezone=True),
        default=sa.func.now(),
        server_default=sa.FetchedValue(),
    )
    updated_at = sa.Column(
        psql.TIMESTAMP(timezone=True),
        onupdate=sa.func.now(),
        server_default=sa.FetchedValue(),
        server_onupdate=sa.FetchedValue(),
    )
    deleted_at = sa.Column(
        psql.TIMESTAMP(timezone=True), server_default=sa.FetchedValue(),
    )

================================================================================
ФАЙЛ: internal/service/__init__.py
================================================================================


================================================================================
ФАЙЛ: internal/service/docs.py
================================================================================

from typing import Dict, Any

import sqlalchemy as sa
from sqlalchemy.orm import selectinload
from internal.dto.docs import DocsCreate, DocsRead
from internal.entity.docs import Docs, MilvusDocs
from internal.service.service import Service
from package.minio.main import MinioClient


class DocsService(Service[Docs]):
    async def transaction_to_minio(self, minio_client: MinioClient, dto: DocsCreate, bucket: str, file) -> Docs:
        """
        Handles the process of saving a transaction in PostgreSQL and uploading a file
        to an MinIO bucket. This involves creating a model instance using the input data,
        storing it in the database, uploading the file, and rolling back the MinIO upload
        if database commit fails.

        Parameters:
            minio_client (MinioClient): The client used for interacting with the MinIO storage service.
            dto (DocsCreate): The data transfer object containing the fields necessary for creating the database entry.
            bucket (str): The name of the MinIO bucket where the file will be uploaded.
            file: The file object that is to be uploaded to MinIO.

        Raises:
            Exception: Propagates any exceptions that occur during the transaction and rollback processes.

        Returns:
            instance: The created database object that successfully corresponds to the input data and uploaded file.
        """
        instance = self.model(**dto.dict())
        self.session.add(instance)

        # Сначала пытаемся загрузить в MinIO
        minio_client.upload_file_to_bucket(
            bucket_name=bucket,
            file_io=file,
            object_name=instance.name,
        )

        try:
            # Если успешно загрузили в MinIO, фиксируем транзакцию в Postgres
            await self.session.commit()
        except Exception as e:
            # Если commit в Postgres не удался, удаляем уже загруженный файл
            minio_client.delete_file_from_bucket(bucket, instance.name)
            raise e  # Перебрасываем исключение
        return instance

    async def create_docs_and_milvus(self, dto: DocsCreate, milvus_ids: list[int]) -> dict[str, Any]:
        """
        Creates a record in the `Docs` table and a related record in the
        `MilvusDocs` table within a single transactional context. All operations
        are committed atomically, ensuring data consistency. In case of an
        exception, the entire transaction is rolled back.

        Args:
            dto (DocsCreate): Data transfer object containing attributes for the
                `Docs` model.
            milvus_ids (list[int]): Identifier to associate the `Docs` record with a
                `MilvusDocs` record.

        Raises:
            Exception: Any exceptions that occur during execution, resulting in a
                rollback of the transaction.

        Returns:
            DocsCreate: Instance of the newly created `Docs` record.
        """

        # Создаём запись в таблице Docs
        instance = self.model(**dto.dict())
        self.session.add(instance)
        await self.session.commit()
        instance_set = [
            MilvusDocs(docs_id=instance.id, milvus_id=pk) for pk in milvus_ids
        ]
        self.session.add_all(instance_set)
        await self.session.commit()

        # Ожидание фиксации в рамках транзакции (автоматически сделает commit в конце контекста)

        return DocsRead.model_validate(instance).model_dump()


class MilvusDocsService(Service[MilvusDocs]):

    async def get_one_or_none(self, milvus_id: int, *where, **filter_by):
        """
        Retrieve a single record based on the given conditions or return None if no matching record is found.

        The method executes a database query using SQLAlchemy to select a record from the model
        based on the milvus_id, optional filtering conditions, and optional filtering parameters.
        It allows for a flexible query construction using where clauses and filter_by conditions.

        Args:
            milvus_id (int): The unique identifier to filter the record by.
            *where: Additional positional filtering conditions to apply to the query.
            **filter_by: Additional named parameters for filtering the query.

        Returns:
            The selected scalar record if it exists, otherwise None.
        """
        return await self.session.scalar(
            sa.select(self.model).options(selectinload(MilvusDocs.docs)).filter_by(
                milvus_id=milvus_id, **filter_by,
            ).where(*where),
        )

================================================================================
ФАЙЛ: internal/service/service.py
================================================================================

from typing import Generic, TypeVar, get_args
from uuid import UUID

import sqlalchemy as sa
from fastapi import Depends
from pydantic import BaseModel
from sqlalchemy.ext.asyncio import AsyncSession

from internal.entity.base import Base
from internal.usecase.utils import exceptions, get_session
from package.pagination import Params

T = TypeVar('T')


class Service(Generic[T]):  # noqa: WPS214, WPS338

    model: Base

    def __init__(self, session: AsyncSession = Depends(get_session)):
        self.session = session

    def __init_subclass__(cls):
        cls.model = get_args(cls.__orig_bases__[0])[0]

    def raise_not_found(self) -> None:
        raise exceptions.HTTP_404_NOT_FOUND(
            'Not found {0}'.format(self.model.__name__),
        )

    async def get_one(self, id: UUID, *where, **filter_by) -> T:
        instance = await self.get_one_or_none(id, *where, **filter_by)
        if instance is None:
            self.raise_not_found()

        return instance

    async def get_one_or_none(self, id: UUID, *where, **filter_by) -> T | None:
        return await self.session.scalar(
            sa.select(self.model).filter_by(
                id=id, deleted_at=None, **filter_by,
            ).where(*where),
        )

    async def select(self, dto: Params, *where, **filter_by) -> tuple[list[T], int]:
        instance_set = await self.session.scalars(
            sa.select(self.model).where(*where).filter_by(
                deleted_at=None, **filter_by,
            ).limit(dto.limit).offset(dto.offset),
        )
        return instance_set.unique().all(), await self.count(*where)

    async def select_all(self, *where, **filter_by) -> list[T]:
        instance_set = await self.session.scalars(
            sa.select(self.model).filter_by(deleted_at=None, **filter_by).where(*where),
        )
        return instance_set.unique().all()

    async def create(self, dto: BaseModel) -> T:
        instance = self.model(**dto.dict())
        self.session.add(instance)
        await self.session.commit()

        return instance

    async def create_many(self, dto_set: list[BaseModel]) -> list[T]:
        instance_set = [self.model(**dto.dict()) for dto in dto_set]
        self.session.add_all(instance_set)
        await self.session.commit()

        return instance_set

    async def update(self, id: UUID, **values) -> T:
        await self.session.execute(
            sa.update(self.model).filter_by(id=id).values(**values),
        )
        await self.session.commit()

        return await self.get_one(id)

    async def delete(self, id: UUID) -> None:
        await self.get_one(id)

        await self.session.execute(
            sa.update(self.model).filter_by(id=id).values(deleted_at=sa.func.now()),
        )
        await self.session.commit()

    async def count(self, *where, **filter_by) -> int:
        return await self.session.scalar(
            sa.select(sa.func.count(self.model.id)).filter_by(
                **filter_by, deleted_at=None,
            ).where(*where),
        )

================================================================================
ФАЙЛ: internal/service/utils.py
================================================================================

from contextlib import asynccontextmanager

from internal.config.modules.database import get_database_client


@asynccontextmanager
async def get_service(service_class):
    """
    An asynchronous context manager for obtaining and managing a service instance.

    This function is designed to provide a service instance of the specified
    service class, utilizing a session created by the asynchronous database client.
    It ensures proper cleanup of the session once the context is exited. This
    function supports dependency injection by creating the service with the session
    as its initialization parameter.

    Args:
        service_class: The class of the service to be managed. This should be a
        subclass of `Service`.

    Yields:
        An instance of the provided `service_class`, initialized with the session.

    Raises:
        Any exceptions raised within the context will propagate back to the caller.
    """
    async for session in get_database_client():
        service = service_class(session)
        try:
            yield service
        finally:
            await session.close()

================================================================================
ФАЙЛ: internal/usecase/__init__.py
================================================================================


================================================================================
ФАЙЛ: internal/usecase/utils/__init__.py
================================================================================

from .exceptions import *
from .mocks import get_session
from .responses import (
    ResponseExample,
    ResponseSchema,
    SuccessfulResponse,
)

================================================================================
ФАЙЛ: internal/usecase/utils/exceptions/__init__.py
================================================================================

from .exceptions import (
    HTTP_400_BAD_REQUEST,
    HTTP_401_UNAUTHORIZED,
    HTTP_403_FORBIDDEN,
    HTTP_404_NOT_FOUND,
)
from .handlers import (
    database_error_handler,
    database_not_found_handler,
    http_exception_handler,
)

================================================================================
ФАЙЛ: internal/usecase/utils/exceptions/exceptions.py
================================================================================

from fastapi import HTTPException, status


class HTTPException(HTTPException):  # noqa: WPS440

    def __call__(self, detail: str) -> HTTPException:
        return HTTPException(self.status_code, detail)


HTTP_400_BAD_REQUEST = HTTPException(
    detail='Bad Request',
    status_code=status.HTTP_400_BAD_REQUEST,
)
HTTP_401_UNAUTHORIZED = HTTPException(
    detail='Unauthorized',
    status_code=status.HTTP_401_UNAUTHORIZED,
)
HTTP_403_FORBIDDEN = HTTPException(
    detail='Forbidden',
    status_code=status.HTTP_403_FORBIDDEN,
)
HTTP_404_NOT_FOUND = HTTPException(
    detail='Not Found',
    status_code=status.HTTP_404_NOT_FOUND,
)

================================================================================
ФАЙЛ: internal/usecase/utils/exceptions/handlers.py
================================================================================

from fastapi import HTTPException, Request, status
from fastapi.responses import JSONResponse
from sqlalchemy.exc import DBAPIError, NoResultFound


async def database_error_handler(
        _: Request, exc: DBAPIError,
) -> JSONResponse:
    detail = str(exc.orig).split('DETAIL:  ')[-1].rstrip('.')
    return JSONResponse(
        content={'detail': detail},
        status_code=status.HTTP_400_BAD_REQUEST,
    )


async def database_not_found_handler(
        _: Request, exc: NoResultFound,
) -> JSONResponse:
    return JSONResponse(
        content={'detail': str(exc)},
        status_code=status.HTTP_404_NOT_FOUND,
    )


async def http_exception_handler(
        _: Request, exc: HTTPException,
) -> JSONResponse:
    response = JSONResponse(
        content={'detail': exc.detail},
        status_code=exc.status_code,
    )
    if exc.headers is not None:
        response.init_headers(exc.headers)

    return response

================================================================================
ФАЙЛ: internal/usecase/utils/mocks.py
================================================================================

def get_session():
    raise NotImplementedError

================================================================================
ФАЙЛ: internal/usecase/utils/responses.py
================================================================================

from typing import Any, Dict, TypedDict

from fastapi import status
from fastapi.responses import Response
from typing_extensions import NotRequired

from fastapi.responses import JSONResponse


class DynamicResponse:

    @staticmethod
    def schema(status_code: int, description: str, example: dict) -> dict:
        """
        This static method generates a structured dictionary schema for an HTTP response, which includes the status code,
        description of the response, and an example payload. It organizes the data in a manner consistent with common API
        documentation formats.

        Arguments:
            status_code (int): The HTTP status code to be included in the response schema.
            description (str): A textual description associated with the provided status code.
            example (dict): A sample JSON payload illustrating the response structure and data.

        Returns:
            dict: A dictionary structure representing the HTTP response schema, formatted for API documentation purposes.
        """
        return {
            status_code: {
                'description': description,
                'content': {
                    'application/json': {
                        'data': example,
                    },
                },
            },
        }

    @staticmethod
    def create(
            status_code: int,
            detail: str,
            description: str = '',
            example: dict = None,
    ) -> JSONResponse:
        """
            Creates a JSONResponse object configured with a specific status code,
            description, detail message, and optional example content. This method
            provides a structured response format for JSON-based API responses.

            Parameters:
            status_code: int
                The HTTP status code to include in the response.
            description: str
                A brief description of the response.
            detail: str
                A detailed message or additional information to include in the response content.
            example: dict, optional
                Example data to include in the response. If not provided, a default
                dictionary with only the detail message will be used.

            Returns:
            JSONResponse
                A JSONResponse object containing the provided status code, description,
                and structured content with the example data.
        """
        if example is None:
            example = {}

        # Наполняем пример данными
        example['detail'] = detail

        # Возвращяем JSONResponse в требуемом формате
        return JSONResponse(
            status_code=status_code,
            content={
                "status_code": status_code,
                "description": description,
                "content": {
                    "application/json": {
                        "data": example,
                    },
                },
            },
        )


class ResponseExample(TypedDict):
    detail: NotRequired[str]


class ResponseSchema(dict):  # noqa: WPS600

    def __init__(
            self,
            status_code: int,
            description: str,
            example: ResponseExample,
    ) -> None:
        self.example = example
        self.status_code = status_code
        self.description = description
        super().__init__(self.schema(
            example=example,
            status_code=status_code,
            description=description,
        ))

    def __call__(self, detail: str = '', description: str = ''):
        example = self.example.copy()
        example['detail'] = detail or example['detail']
        return self.schema(
            example=example,
            status_code=self.status_code,
            description=description or self.description,
        )

    @classmethod
    def schema(
            cls,
            status_code: int,
            description: str,
            example: ResponseExample,
    ) -> Dict[int, Dict[str, Any]]:
        return {
            status_code: {
                'description': description,
                'content': {
                    'application/json': {
                        'data': example,
                    },
                },
            },
        }


class SuccessfulResponse(Response):

    def __init__(self, status_code: int = status.HTTP_204_NO_CONTENT):
        super().__init__(status_code=status_code)


HTTP_200_OK_REQUEST = ResponseSchema(
    status_code=status.HTTP_200_OK,
    description='OK',
    example=ResponseExample(detail='OK'),
)

HTTP_400_BAD_REQUEST = ResponseSchema(
    status_code=status.HTTP_400_BAD_REQUEST,
    description='Bad Request',
    example=ResponseExample(detail='Bad Request'),
)
HTTP_403_FORBIDDEN = ResponseSchema(
    status_code=status.HTTP_403_FORBIDDEN,
    description='Forbidden',
    example=ResponseExample(detail='Forbidden'),
)
HTTP_404_NOT_FOUND = ResponseSchema(
    status_code=status.HTTP_404_NOT_FOUND,
    description='Not Found',
    example=ResponseExample(detail='Not Found'),
)

================================================================================
ФАЙЛ: internal/usecase/utils/tools.py
================================================================================

def convert_size(size: int, unit: str) -> float:
    """
    Convert a file size from bytes into a human-readable string representation based on the provided
    unit of measurement. A specific representation, expressed in bytes, kilobytes, megabytes, gigabytes,
    or terabytes, is returned formatted to two decimal places.

    Args:
        size (int): File size in bytes to be converted.
        unit (str): Unit index indicating the conversion level (0 for bytes, 1 for kilobytes,
                    2 for megabytes, 3 for gigabytes, 4 for terabytes).

    Returns:
        str: The file size formatted as a string with the specified unit.
    """
    return float("{:.2f}".format(
        size / 1024 ** 0 if unit == 'B' else 1 if unit == 'KB' else 2 if unit == 'MB' else 3 if unit == 'GB' else 4
    ))


================================================================================
ФАЙЛ: main.py
================================================================================

import asyncio
import logging

from package.milvus import MilvusClient

# Конфигурируем логирование
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Конфигурация
MILVUS_HOST = 'localhost'
MILVUS_PORT = '19530'
COLLECTION_NAME = 'pdf_embeddings'
DIMENSION = 1536


async def main():
    # Настройка Milvus
    milvus_client = MilvusClient(host=MILVUS_HOST, port=MILVUS_PORT)
    # result = milvus_client.get_all_vectors(collection_name=COLLECTION_NAME)
    # for item in result:
    #     print(item)
    milvus_client.drop_collection('pdf_embeddings')
    milvus_client.create_collection('pdf_embeddings', DIMENSION)


if __name__ == '__main__':
    asyncio.run(main())

================================================================================
ФАЙЛ: make/compose.Makefile
================================================================================

.PHONY: compose-build
compose-build: ## Build or rebuild services
	docker compose -f docker/docker-compose.yml --env-file $(ENV_FILE) build

.PHONY: compose-up-build
compose-up-build: ## Create and start containers
	docker compose -f docker/docker-compose.yml --env-file $(ENV_FILE) up -d --build

.PHONY: compose-up
compose-up: ## Create and start containers
	docker compose -f docker/docker-compose.yml --env-file $(ENV_FILE) up -d

.PHONY: compose-migration
compose-migration: ## Create and start containers
	docker compose -f docker/docker-compose.yml --env-file $(ENV_FILE) up migration --exit-code-from migration

.PHONY: compose-logs
compose-logs: ## View output from containers
	docker compose -f docker/docker-compose.yml logs

.PHONY: compose-ps
compose-ps: ## List containers
	docker compose -f docker/docker-compose.yml ps

.PHONY: compose-ls
compose-ls: ## List running compose projects
	docker compose -f docker/docker-compose.yml ls

.PHONY: compose-start
compose-start: ## Start services
	docker compose -f docker/docker-compose.yml --env-file $(ENV_FILE) start

.PHONY: compose-restart
compose-restart: ## Restart services
	docker cp ./ tarvos-api:/code/
	docker compose -f docker/docker-compose.yml restart

.PHONY: compose-stop
compose-stop: ## Stop services
	docker compose -f docker/docker-compose.yml stop

.PHONY: compose-down
compose-down: ## Stop and remove containers, networks
	docker compose -f docker/docker-compose.yml down --remove-orphans

================================================================================
ФАЙЛ: make/docker.Makefile
================================================================================

.PHONY: docker-clean
docker-clean: ## Remove unused data
	docker system prune -a

================================================================================
ФАЙЛ: make/lint.Makefile
================================================================================

.PHONY: lint
lint: ## Run linters
	make lint-isort
	make lint-flake

.PHONY: lint-isort
lint-isort: ## Run isort linter
	isort .

.PHONY: lint-flake8
lint-flake:  ## Run flake8 linter
	flake8

================================================================================
ФАЙЛ: make/migration.Makefile
================================================================================

.PHONY: migrate-create
migrate-create: ## Create new migration
	alembic revision --autogenerate -m $(name)

.PHONY: migrate-history
migrate-history: ## Migration history
	alembic history

.PHONY: migrate-up
migrate-up: ## Migration up
	alembic upgrade head

.PHONY: migrate-down
migrate-down: ## Migration down
	alembic downgrade -1

================================================================================
ФАЙЛ: make/run.Makefile
================================================================================

.PHONY: run
run: ## Run application
	gunicorn \
	    --bind $(APP_HOST):$(APP_PORT_INTERNAL) \
		--worker-class uvicorn.workers.UvicornWorker \
		--workers $(APP_WORKERS) \
		--log-level $(APP_LOG_LEVEL) \
		--chdir cmd/app \
		main:app

.PHONY: run-dev
run-dev: ## Run application in development mode
	uvicorn --app-dir cmd/app main:app --reload --host 0.0.0.0

================================================================================
ФАЙЛ: make/test.Makefile
================================================================================

.PHONY: test
test: ## Run pytest
	poetry run pytest -rs --junitxml=reports/test-report.xml

.PHONY: test-coverage
test-coverage: ## Run pytest coverage
	poetry run coverage run -m pytest -rs --junitxml=reports/test-report.xml
	poetry run coverage report
	poetry run coverage xml -o reports/coverage.xml
	poetry run coverage html -d reports/coverage/

================================================================================
ФАЙЛ: migrations/README
================================================================================

Generic single-database configuration.

================================================================================
ФАЙЛ: migrations/script.py.mako
================================================================================

"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ''}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else 'pass'}


def downgrade() -> None:
    ${downgrades if downgrades else 'pass'}

================================================================================
ФАЙЛ: migrations/versions/c9309fddd1ae_initial.py
================================================================================

"""initial

Revision ID: c9309fddd1ae
Revises: 
Create Date: 2025-01-06 02:57:11.472461

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
from sqlalchemy.schema import FetchedValue

# revision identifiers, used by Alembic.
revision: str = 'c9309fddd1ae'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('docs',
                    sa.Column('name', sa.String(length=255), nullable=True),
                    sa.Column('checksum', sa.BigInteger(), nullable=True),
                    sa.Column('s3_briefly', sa.String(length=255), nullable=True),
                    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=FetchedValue(),
                              nullable=True),
                    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), server_default=FetchedValue(),
                              nullable=True),
                    sa.Column('deleted_at', postgresql.TIMESTAMP(timezone=True), server_default=FetchedValue(),
                              nullable=True),
                    sa.Column('id', sa.UUID(), server_default=sa.text('gen_random_uuid()'), nullable=False),
                    sa.PrimaryKeyConstraint('id')
                    )
    op.create_index(op.f('ix_docs_id'), 'docs', ['id'], unique=False)
    op.create_table('milvus_docs',
                    sa.Column('milvus_id', sa.BigInteger(), nullable=False),
                    sa.Column('docs_id', sa.UUID(), nullable=False),
                    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=FetchedValue(),
                              nullable=True),
                    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), server_default=FetchedValue(),
                              nullable=True),
                    sa.Column('deleted_at', postgresql.TIMESTAMP(timezone=True), server_default=FetchedValue(),
                              nullable=True),
                    sa.ForeignKeyConstraint(['docs_id'], ['docs.id'], ),
                    sa.PrimaryKeyConstraint('milvus_id')
                    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('milvus_docs')
    op.drop_index(op.f('ix_docs_id'), table_name='docs')
    op.drop_table('docs')
    # ### end Alembic commands ###

================================================================================
ФАЙЛ: package/__init__.py
================================================================================


================================================================================
ФАЙЛ: package/celery/__init__.py
================================================================================


================================================================================
ФАЙЛ: package/celery/tasks.py
================================================================================

from celery import Task
import requests

from internal.config.settings import settings

webhook_url = settings.TELEGRAM_WEBHOOK


class MyTaskWithSuccess(Task):
    # Поведение при успешном завершении задачи
    def on_success(self, retval, task_id, args, kwargs):
        _, user_id, document = retval
        print(f"Document: {document}, User ID: {user_id}")
        result = requests.post(webhook_url, json={"file_url": f'https://cdn.student-space.ru/{document}',
                                                  "user_id": user_id})
        super().on_success(retval, task_id, args, kwargs)

    # Поведение при ошибке (добавлено для полноты примера)
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        print(f"Task {task_id} failed with exception: {exc}")
        super().on_failure(exc, task_id, args, kwargs, einfo)

================================================================================
ФАЙЛ: package/celery/worker.py
================================================================================

import uuid
import asyncio
from io import BytesIO

from markdown_pdf import MarkdownPdf, Section
from celery import Celery

from internal.config import get_milvus_client, get_gpt_client, get_minio_client
from internal.config.settings import settings, buckets
from internal.dto.docs import DocsCreate, MilvusDocsRead
from internal.service.docs import DocsService, MilvusDocsService
from internal.service.utils import get_service
from package.celery.tasks import MyTaskWithSuccess
from package.pdf import PDFProcessor

celery = Celery(__name__, broker=str(settings.CELERY_BROKER_URL), backend=str(settings.CELERY_RESULT_BACKEND))

minio_client = get_minio_client()
chatgpt_client = get_gpt_client()
milvus_client = get_milvus_client()


def process_pdf_and_extract(file_stream: BytesIO, start_page: int = 0):
    """
    Process a PDF file and extract text.

    This function utilizes a PDF processing utility to extract text from a given
    PDF file stream. It initializes a PDFProcessor object with the provided file
    stream, processes the PDF starting from the specified page, and extracts the
    text content from all processed pages. The extracted text is returned as a
    single concatenated string with line breaks.

    Args:
        file_stream (BytesIO): A binary stream representing the PDF file.
        start_page (int): The page number to start processing the PDF from. Defaults
            to 0.

    Returns:
        str: The extracted text from the PDF, concatenated with line breaks.
    """
    pdf_processor = PDFProcessor(file_stream)
    pdf_processor.process_pdf(start_page=start_page, end_page=pdf_processor.pages)
    return '\n'.join(pdf_processor.extract())


def handle_embeddings_and_texts(chunks: list, collection_name: str, prompt_type: str):
    """
    Handles the embedding creation from chunks, searches for matching vectors in
    Milvus storage, and processes text data from the input chunks if no sufficient
    match is found.

    Combines the functionality of generating embeddings with the external API,
    querying the vector database for relevant matches, validating match thresholds,
    and generating fallback or additional results for unmatched embeddings and texts.

    Parameters:
        chunks (list): A list of input text chunks to process for embedding and text
        handling.

        collection_name (str): The name of the embedding collection in the Milvus
        database to perform the vector search.

        prompt_type (str): The type of the prompt to be used when generating embeddings.

    Returns:
        Tuple: A tuple containing the generated embedding, search results from the
        Milvus database, and optionally, a tuple of new embeddings and concatenated
        processed text if no sufficient match is found.
    """
    if prompt_type is not None:
        chatgpt_client.system_prompt = prompt_type
    embedding = chatgpt_client.create_embeddings(chunks)
    results = milvus_client.search_vectors(collection_name, query_vector=embedding, limit=1)
    if results and results[0]['distance'] >= 0.9:
        return embedding, results, None
    new_embeddings = [embedding]
    texts = ''.join(chatgpt_client.send_message(chunk) for chunk in chunks)
    return embedding, results, (new_embeddings, texts)


@celery.task(base=MyTaskWithSuccess, name='process_document')
def process_document(filename: str, bucket: str, user_id: str, prompt_type: str):
    """
    Asynchronous task for processing a document file stored in a MinIO bucket. The task includes
    retrieval of the file, processing it to extract text, preparing embeddings for the text chunks,
    and storing the final results in a Milvus database. Additionally, it generates a new PDF file
    from the processed text and uploads it back to a specified MinIO bucket.

    Args:
        filename (str): Name of the file to be processed from the MinIO bucket.
        bucket (str): Name of the MinIO bucket where the file is stored.
        user_id (str): ID of the user processing the document.
        prompt_type (str): Type of the prompt for the user to prompt.

    Returns:
        dict: A dictionary representing the result created in the Milvus database.

    Raises:
        KeyError: Raised in case of unexpected missing buckets during operations.
        StorageException: Raised on failure to interact with MinIO client.
        ProcessingException: Raised for any issues in text processing or PDF creation.
        DatabaseException: Raised for errors occurring during interactions with Milvus.
    """
    file, _ = minio_client.get_file_from_bucket(bucket_name=bucket, object_name=filename)
    file_stream = BytesIO(file)

    # Обработка PDF
    long_text = process_pdf_and_extract(file_stream)

    # Разбивка текста на чанки
    chunks = chatgpt_client.split_text_into_chunks(long_text, chunk_size=chatgpt_client.max_tokens)

    # Работа с эмбеддингами и текстами
    embedding, results, embeddings_and_texts = handle_embeddings_and_texts(
        chunks,
        settings.COLLECTION_NAME,
        prompt_type)

    if embeddings_and_texts is None:
        for milvus_object in results:
            result = asyncio.run(__get_docs_milvus(milvus_object['id']))
            if result is None:
                continue
            return result, user_id, result['docs']['s3_briefly']

    new_embeddings, texts = embeddings_and_texts
    ids = milvus_client.insert_vectors(settings.COLLECTION_NAME, new_embeddings)

    # Генерация PDF и загрузка в MinIO
    pdf = MarkdownPdf(toc_level=3)
    pdf.add_section(Section(texts, toc=False))
    pdf.writer.close()
    pdf.out_file.seek(0)
    object_name = f"{uuid.uuid4()}.pdf"
    new_bucket = buckets.get('pdf')
    minio_client.upload_file_to_bucket(file_io=pdf.out_file, bucket_name=new_bucket, object_name=object_name)
    result = asyncio.run(__create_docs_milvus(ids, object_name, new_bucket))
    return result, user_id, result['s3_briefly']


async def __create_docs_milvus(
        milvus_ids: list[int],
        doc_name: str,
        bucket: str,
):
    async with get_service(DocsService) as docs_service:
        s3_briefly = f"{bucket}/{doc_name}"
        dto_doc = DocsCreate(
            name=doc_name,
            s3_briefly=s3_briefly,
        )
        result = await docs_service.create_docs_and_milvus(dto_doc, milvus_ids)
        return result


async def __get_docs_milvus(milvus_id: int):
    async with get_service(MilvusDocsService) as milvus_docs_service:
        result = await milvus_docs_service.get_one_or_none(milvus_id)
        return MilvusDocsRead.model_validate(result).model_dump()

================================================================================
ФАЙЛ: package/milvus/__init__.py
================================================================================

from .main import MilvusClient

================================================================================
ФАЙЛ: package/milvus/main.py
================================================================================

import logging

from pymilvus import (
    Collection,
    CollectionSchema,
    DataType,
    FieldSchema,
    connections,
    has_collection,
)


class MilvusClient:
    def __init__(self, host: str = 'localhost', port: str = '19530'):
        """
        Initializes a new instance of MilvusClient.

        Args:
            host (str): The host address of the Milvus server.
            port (str): The port number of the Milvus server.
        """
        self.host = host
        self.port = port
        self.connection_alias = 'default'
        self._connect()

    def _connect(self):
        """
        Establish a connection to the Milvus server.
        """
        connections.connect(alias=self.connection_alias, host=self.host, port=self.port)

    def create_collection(self, collection_name: str, dim: int, metric_type: str = 'COSINE'):
        """
        Create a collection in Milvus if it does not already exist.

        Args:
            collection_name (str): The name of the collection.
            dim (int): The dimensionality of the vectors.
            metric_type (str): The distance metric type (COSINE, L2, etc.).
        """
        # Проверка, существует ли коллекция
        if not has_collection(collection_name):
            fields = [
                FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name='vector', dtype=DataType.FLOAT_VECTOR, dim=dim),
            ]
            schema = CollectionSchema(fields, description=f'Collection for {collection_name}')
            collection = Collection(name=collection_name, schema=schema)

            # Создаем индекс для коллекции
            index_params = {
                'index_type': 'HNSW',
                'metric_type': metric_type,
                'params': {'M': 32, 'efConstruction': 400},
            }
            collection.create_index(field_name='vector', index_params=index_params)
            logging.info(f'Коллекция {collection_name} успешно создана.')
        else:
            logging.info(f'Коллекция {collection_name} уже существует.')

    def insert_vectors(self, collection_name: str, vectors: list[list[float]]) -> list[int]:
        """
        Insert vectors into a collection with auto-incremented IDs.

        Args:
            collection_name (str): The name of the collection.
            vectors (list[list[float]]): List of vectors to insert.
        """
        collection = Collection(collection_name)

        mutation_result = collection.insert(vectors)

        generated_ids = mutation_result.primary_keys

        logging.info(f'Inserted {len(vectors)} vectors into collection {collection_name}')
        return generated_ids

    def search_vectors(self, collection_name: str, query_vector: list[list[float]], limit: int = 5):
        """
        Search for similar vectors in a collection.

        Args:
            collection_name (str): The name of the collection.
            query_vector: list((list[float])): The vector to search for.
            limit (int): The number of top results to return.

        Returns:
            list[dict]: List of search results with IDs and distances.
        """
        collection = Collection(collection_name)
        collection.load()
        search_params = {'metric_type': 'COSINE', 'params': {'ef': 50}}
        results = collection.search(
            data=query_vector,
            anns_field='vector',
            param=search_params,
            limit=limit,
            output_fields=['id'],
        )
        output = [
            {'id': hit.id, 'distance': hit.distance} for hits in results for hit in hits
        ]
        logging.info(f'Search completed. Found {len(output)} results.')
        return output

    def delete_vector(self, collection_name: str, vector_id: int):
        """
        Delete a vector from a collection by its ID.

        Args:
            collection_name (str): The name of the collection.
            vector_id (int): The ID of the vector to delete.
        """
        collection = Collection(collection_name)
        expr = f'id == {vector_id}'
        collection.delete(expr)
        logging.info(f'Vector with ID {vector_id} deleted from collection {collection_name}')

    def drop_collection(self, collection_name: str):
        """
        Drop a collection from Milvus.

        Args:
            collection_name (str): The name of the collection.
        """
        collection = Collection(collection_name)
        collection.drop()
        logging.info(f'Collection {collection_name} dropped.')

    def get_all_vectors(self, collection_name: str):
        """
        Получает все эмбеддинги и их ID из указанной коллекции.

        Args:
            collection_name (str): Имя коллекции.

        Returns:
            list[dict]: Список всех векторов с их ID.
        """
        collection = Collection(collection_name)
        collection.load()

        # Запрашиваем все данные из коллекции
        results = collection.query(expr='id != 0', output_fields=['id', 'vector'], liimit=100)

        logging.info(f'Получено {len(results)} записей из коллекции {collection_name}')
        return results

================================================================================
ФАЙЛ: package/minio/__init__.py
================================================================================


================================================================================
ФАЙЛ: package/minio/main.py
================================================================================

import logging
from urllib3 import PoolManager
from minio import Minio


class MinioClient(object):

    def __init__(self, endpoint: str, access_key: str, secret_key: str):
        """
        Initializes a new instance of a class with the necessary credentials and
        endpoint configuration to perform operations.

        Args:
            endpoint (AnyUrl): The base URL for the endpoint to connect to.
            access_key (str): The public key required for authentication.
            secret_key (str): The private key used for secure access.

        """
        self.__secret_key = secret_key
        self._access_key = access_key
        self.uri = endpoint
        self.__client: Minio | None = None
        self._http_client = PoolManager(
            num_pools=10,  # Максимальное количество пулов
            maxsize=10,  # Максимальное количество одновременных подключений
            timeout=10.0  # Таймаут для подключения в секундах (можно указать объект Timeout)
        )

    @property
    def connection(self) -> Minio:
        """
        Returns a Minio client connection. This property checks if the client
        is initialized, if not, it creates a new Minio client using the provided URI,
        access key, and secret key, and then returns the client. This ensures a
        consistent and efficient way to manage the Minio client connection.

        Attributes:
            __client (Minio): The Minio client used for connecting to the storage.
            uri (str): The URI of the Minio server.
            _access_key (str): The access key for authentication with the Minio server.
            __secret_key (str): The secret key for authentication with the Minio server.

        Returns:
            Minio: The Minio client connection.
        """
        if self.__client is None:
            self.__client = Minio(self.uri,
                                  access_key=self._access_key,
                                  secret_key=self.__secret_key,
                                  secure=False,
                                  http_client=self._http_client
                                  )
            logging.info(f'Set connection to minio by address: {self.uri} ')
        return self.__client

    def get_or_create_bucket(self, bucket_name: str) -> (bool, str):
        """
        Manages operations related to bucket creation and management in a storage
        service.

        Attributes:
            connection: Represents the connection to the storage service.
        """
        logging.info(f'Get or create bucket: {bucket_name}')
        bucket = self.connection.bucket_exists(bucket_name)
        if bucket is None:
            logging.info(f'Create bucket: {bucket_name}')
            self.connection.make_bucket(bucket_name=bucket_name)
        return bucket, bucket_name

    def delete_file_from_bucket(self, bucket_name: str, object_name: str) -> None:
        _, bucket = self.get_or_create_bucket(bucket_name)
        self.connection.remove_object(bucket_name, object_name)

    def upload_file_to_bucket(self, bucket_name: str, file_io, object_name: str) -> None:
        """
        Uploads a file from io format to a specified bucket in the storage service.

        Args:
            bucket_name (str): The name of the bucket where the file will be stored.
            file_io (IO): The file-like object to upload.
            object_name (str): The name of the object in the bucket.
        """
        _, bucket = self.get_or_create_bucket(bucket_name)
        self.connection.put_object(
            bucket_name=bucket,
            object_name=object_name,
            data=file_io,
            length=-1,  # Specify the file length, or for unknown length specify -1 for stream
            part_size=10 * 1024 * 1024  # You can adjust the part size
        )
        logging.info(f'Upload file: {object_name} to bucket: {bucket}')

    def get_file_from_bucket(self, bucket_name: str, object_name: str) -> (bytes, str):
        """
        Downloads a file from a specified bucket and returns its content and URL.

        Args:
            bucket_name (str): The name of the bucket from which to download the file.
            object_name (str): The name of the object to download.

        Returns:
            Tuple: A tuple containing the file content (as bytes) and the URL for accessing the file.
        """
        # Getting the object from the bucket
        response = self.connection.get_object(bucket_name, object_name)
        try:
            # Reading the data from the response
            file_data = response.read()
        finally:
            # Make sure to close the response to release the connection
            response.close()
            response.release_conn()

        # Constructing a URL for the object
        url = self.connection.presigned_get_object(bucket_name, object_name)

        logging.info(f'Get file: {object_name} to bucket: {bucket_name}')
        return file_data, url

================================================================================
ФАЙЛ: package/openai/__init__.py
================================================================================

from .prompts import PromptManager
from .client import ChatGPTClient

================================================================================
ФАЙЛ: package/openai/client.py
================================================================================

import logging
from typing import Any, List, Optional, Generator

import tiktoken
from langchain.schema import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from pydantic import SecretStr


class ChatGPTClient(object):
    def __init__(
            self,
            api_key: SecretStr,
            model_name: str = 'gpt-4o-mini',
            embeddings_model_name: str = 'text-embedding-ada-002',
            system_prompt: Optional[str] = None,
            mathematical_percent: Optional[int] = 20,
    ):
        """Initialize the configuration for interacting with OpenAI's GPT-4 and text.

        embedding models. It sets up the necessary components to communicate with the
        OpenAI API, including chat and embeddings model instances, tokenizers, and the
        system prompt if provided. The class handles token limit settings based on the
        specified models and manages initial chat history.

        Args:
            api_key: A secret string representing the OpenAI API key required for
                authentication.
            model_name: The name of the chat model to be used, defaulting to 'gpt-4'.
                Determines which model to use for chat operations.
            embeddings_model_name: The name of the embeddings model to be used,
                defaulting to 'text-embedding-ada-002'. It defines which model is to be
                utilized for handling text embeddings.
            system_prompt: An optional string for setting a system-level prompt
                message. If provided, it initializes the conversation context with this
                prompt.
            mathematical_percent: An optional integer defining a mathematical parameter,
                defaulting to 100. This parameter may be used for internal calculations
                or configurations.

        """
        self._api_key = api_key
        self.model_name = model_name
        self.math_p = mathematical_percent
        self.embeddings_model_name = embeddings_model_name
        self.chat_model = ChatOpenAI(
            openai_api_key=self._api_key,
            model_name=self.model_name,
        )
        self.embeddings_model = OpenAIEmbeddings(
            openai_api_key=self._api_key,
            model=self.embeddings_model_name,
        )
        self.chat_history = []

        # Явно указываем токенизаторы
        self.tokenizer = tiktoken.get_encoding('cl100k_base')
        self.embeddings_tokenizer = tiktoken.get_encoding('cl100k_base')

        # Установка системного промпта, если он предоставлен
        self.system_prompt = system_prompt
        if self.system_prompt:
            system_message = SystemMessage(content=self.system_prompt)
            self.chat_history.append(system_message)

        # Установка лимитов токенов в зависимости от моделей

        self.token = self.get_model_token_limit(self.model_name)
        self.max_tokens = self.token - int((self.token / 100) * self.math_p)
        self.embeddings_max_tokens = self.get_model_token_limit(self.embeddings_model_name)

    def get_model_token_limit(self, model_name: str) -> int:
        """Retrieve the token limit for a specified model.

        This function takes a model name as input and returns the token limit
        associated with that model. It uses a predefined dictionary to map
        model names to their respective token limits. If the provided model
        name is not found in the dictionary, a default token limit is
        returned.

        Args:
            model_name: The name of the model for which the token limit is
                requested.

        Returns:
            The token limit for the specified model. If the model is not
            found, a default value of 4096 is returned.
        """
        model_token_limits = {
            'gpt-3.5-turbo': 4096,
            'gpt-3.5-turbo-16k': 16384,
            'gpt-4': 8192,
            # 'gpt-4o-mini': 16384,
            'gpt-4o-mini': 2100,  # специально для точного пересказа текстов.
            'gpt-4-32k': 32768,
            'text-embedding-ada-002': 8191,  # Лимит для модели эмбеддингов
        }
        return model_token_limits.get(model_name, 2000)  # По умолчанию 4096, если модель не найдена

    def create_embeddings(self, texts: List[str] | Generator) -> List[Any]:
        """Create embeddings for the provided texts.

        This method processes a list of texts by tokenizing each text and checking
        if the number of tokens is within the specified maximum tokens limit. If a
        text exceeds the token limi, it splits the text into smaller chunks that
        fit the limit. It then generates embeddings for all valid texts or chunks
        using the specified embeddings model.

        Args:
            texts (List[str]): A list of input texts to generate embeddings for.

        Returns:
            List[Any]: A list containing the embeddings of the valid texts.
        """
        valid_texts = []
        for text in texts:
            tokens = self.embeddings_tokenizer.encode(text)
            if len(tokens) <= self.embeddings_max_tokens:
                valid_texts.append(text)
            else:
                chunks = self.split_text_into_chunks(
                    text,
                    self.embeddings_max_tokens,
                    tokenizer=self.embeddings_tokenizer,
                )
                valid_texts.extend(chunks)
        logging.info('Create Embeddings.')
        return self.embeddings_model.embed_documents(valid_texts)

    def tokenize_text(self, text: str, tokenizer=None) -> List[int]:
        """Tokenize the input text using the specified tokenizer.

        If no tokenizer is provided, the default one is used. The function
        returns a list of token IDs that represent the text in a format
        suitable for processing by language models.

        Args:
            text (str): The text to be tokenized.
            tokenizer: The tokenizer instance to use for tokenization. If
                None, the default tokenizer of the class is used.

        Returns:
            List[int]: A list of integer token IDs representing the
            tokenized form of the input text.
        """
        if tokenizer is None:
            tokenizer = self.tokenizer
        tokens = tokenizer.encode(text)
        logging.info('Tokenize text.')
        return tokens

    def split_text_into_chunks(self, text: str, chunk_size: int, tokenizer=None) -> List[str]:  # noqa: WPS210
        """Split the provided text into chunks based on a specified chunk size.

        The text is tokenized using the provided tokenizer (or a default tokenizer),
        divided into segments of tokens, and then each segment is decoded back into a
        text chunk. This function is useful for handling large text by processing it in
        smaller manageable pieces.

        Args:
            text: The input text that needs to be chunked.
            chunk_size: The number of tokens each chunk should contain.
            tokenizer: An optional tokenizer to be used for tokenizing the text. If no
                tokenizer is provided, a default tokenizer is used.

        Returns:
            A list of strings where each string is a chunk of the original text.
        """
        if tokenizer is None:
            tokenizer = self.tokenizer
        tokens = self.tokenize_text(text, tokenizer)
        chunks = []
        for i in range(0, len(tokens), chunk_size):
            chunk_tokens = tokens[i:i + chunk_size]
            chunk_text = tokenizer.decode(chunk_tokens)
            chunks.append(chunk_text)
        logging.info('Split text to chunks.')
        return chunks

    def send_message(self, message: str) -> str:
        """Send a message to a chat model and receive a response.

        This function manages chat history by appending the human message and assistant
        response, and ensures that the token limit for the model is not
        exceeded before sending the message.

        Args:
            message (str): The message content to be sent to the chat model.

        Returns:
            str: The response content from the chat model.
        """
        # Проверяем, не превышает ли сообщение лимит токенов модели
        human_message = HumanMessage(content=message)
        new_message_tokens = len(self.tokenize_text(human_message.content))
        self.trim_chat_history(new_message_tokens)
        self.chat_history.append(human_message)
        assistant_message = self.chat_model.invoke(self.chat_history)
        self.chat_history.append(assistant_message)
        logging.info('Send message to OpenAI client.')
        return assistant_message.content

    def trim_chat_history(self, new_message_tokens_length):
        """Trim the chat history to ensure the total number of tokens does not exceed a predefined maximum.

        This function iterates through the chat history starting from the most recent
        message, adding messages to a trimmed history list until the token limit is reached.

        Args:
            new_message_tokens_length: The number of tokens in the new message
                to be considered alongside the existing chat history.
        """

        total_tokens = new_message_tokens_length
        trimmed_history = []
        # Начинаем с последних сообщений
        for message in reversed(self.chat_history):
            message_tokens = len(self.tokenize_text(message.content))
            if (total_tokens + message_tokens) <= self.max_tokens:
                trimmed_history.insert(0, message)  # Вставляем в начало
                total_tokens += message_tokens
            else:
                break

        # Эта проверка гарантирует, что системное сообщение присутствует в начале
        if self.system_prompt:
            system_message = SystemMessage(content=self.system_prompt)
            trimmed_history.insert(0, system_message)

        self.chat_history = trimmed_history

    def reset_chat_history(self):
        """Manage the chat history including adding system prompts when necessary.

        Attributes:
            chat_history (list): A list that stores the chat history.
            system_prompt (str): A string representing the system prompt to be added
                to chat history, if it exists.
        """
        self.chat_history = []
        # Повторно добавляем системный промпт, если он есть
        if self.system_prompt:
            system_message = SystemMessage(content=self.system_prompt)
            self.chat_history.append(system_message)
        logging.info('Reset chat history.')

================================================================================
ФАЙЛ: package/openai/prompts/__init__.py
================================================================================

from .manager import PromptManager

================================================================================
ФАЙЛ: package/openai/prompts/manager.py
================================================================================

class PromptManager(object):
    """Класс для управления промтами, используемыми для взаимодействия с OpenAI API."""

    def __init__(self):
        self.prompts = {
            'retell': """Ты — ассистент, который составляет краткий пересказ полученного текста.
                Необходимо оставлять основную мысль, терминологию.
                Изложи все понятным языком. Объем должен сократиться примерно в 2 раза от первоначального.""",
            'translate': 'Ты — переводчик. Переведи текст на русский язык. Используй естественный, литературный язык.',
            'questions': 'Ты — аналитик. Найди ключевые слова и фразы из текста. Укажи их в списке, сохраняя контекст.',
            'test': """
Ты — всезнающий человек. Тебе нужно на основе предоставленных материалов выделить ключевые темы и составить материалы для подготовки.
Указания:
	1.	Выдели ключевые темы и понятия.
	2.	Структурируй материал логически: от базовых понятий к сложным.
	3.	Сами вопросы переписывать не нужно.
Формат:
    1.  Название темы: краткое название темы, о чем пойдет речь.
	2.	Термины и определения: краткие пояснения ключевых понятий.
	3.	Основные идеи: тезисы и факты, необходимые для понимания темы.
	4.  Примеры решения задач, если таковые имеются.
Важно:
	•	Исключи вводные слова, приветствия или заключения.
	•	Добавляй разделитель (например, “——————————————————————————————————————————————————”) только в конце каждого блока ответа, а не между его частями.
	•	Пиши сжато и четко, чтобы материал был информативным.
	•	Выделяй текст для удобства зрительного восприятия. Например выделяй название темы, термины и основные идеи.
	•	Если встречаются формулы, пиши их так, чтобы Markdown их разобрал. Лучше писать формулы текстом без выделений и прочего.
	•   Не пиши задачи, если таковых не имеется в материалах.
	""",
        }

    def get_prompt(self, key: str) -> str:
        """Возвращает текст промта по ключу.

        Args:
            key (str): Ключ промта.

        Returns:
            str: Текст промта.

        Raises:
            KeyError: Если ключ отсутствует в списке промтов.
        """
        if key not in self.prompts:
            print('ERRRORRRR')
            return self.prompts['retell']
        return self.prompts[key]

    def add_prompt(self, key: str, prompt: str):
        """Добавляет новый промт.

        Args:
            key (str): Ключ для нового промта.
            prompt (str): Текст промта.
        """
        self.prompts[key] = prompt

================================================================================
ФАЙЛ: package/pagination.py
================================================================================

from dataclasses import dataclass
from functools import wraps
from typing import Any, Callable, Coroutine, Generic, TypeVar

from fastapi import Query
from pydantic.generics import GenericModel

T = TypeVar('T')


@dataclass
class Params(object):  # noqa: WPS110

    page: int = Query(1, ge=1, description='Page number')
    size: int = Query(50, ge=1, le=500, description='Page size')

    @property
    def limit(self) -> int:
        return self.size

    @property
    def offset(self) -> int:
        return self.size * (self.page - 1)


class Page(GenericModel, Generic[T]):
    items: list[T]  # noqa: WPS110

    total: int
    page: int
    size: int


PaginationEndpoint = Callable[[...], Coroutine[Any, Any, tuple[list[T], int]]]
PaginationWrapper = Callable[[...], Coroutine[Any, Any, Page[T]]]


def paginate(func: PaginationEndpoint) -> PaginationWrapper:
    @wraps(func)
    async def wrapper(*args, **kwargs) -> Page[T]:
        dto = kwargs.get('dto')
        items, total = await func(*args, **kwargs)  # noqa: WPS110
        return Page[T](items=items, total=total, page=dto.page, size=dto.size)

    return wrapper

================================================================================
ФАЙЛ: package/pdf/__init__.py
================================================================================

from .main import PDFProcessor

================================================================================
ФАЙЛ: package/pdf/main.py
================================================================================

from typing import Any, BinaryIO, Dict, List, Optional

import pdfplumber
import PyPDF2
from PyPDF2 import PdfFileWriter
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTPage, LTRect, LTTextContainer

from .tools import extract_table, table_converter, text_extraction


class PDFProcessor:
    def __init__(self, pdf_file: BinaryIO):
        """
        Initialize the PDFProcessor class.

        Args:
            pdf_file (BinaryIO): An opened binary file object for the PDF.
        """
        self.pdf_file = pdf_file
        self.text_per_page: Dict[int, Dict[str, Any]] = {}
        self.pdf_reader = PyPDF2.PdfReader(self.pdf_file)

    @property
    def pages(self):
        return len(self.pdf_reader.pages)

    def process_pdf(self, start_page: int = 0, end_page: Optional[int] = None) -> None:
        """
        Processes a range of pages in the PDF, extracting text and formatting.

        Args:
            start_page (int): The starting page number (0-indexed).
            end_page (Optional[int]): The ending page number (0-indexed, exclusive). If None, processes until the last page.
        """
        num_pages = self.pages

        # Если конечная страница не указана или превышает количество страниц, установить её на последнюю страницу
        if end_page is None or end_page > num_pages:
            end_page = num_pages

        # Получаем срез страниц как список PageObject
        pages_slice = self.pdf_reader.pages[start_page:end_page]

        # Обрабатываем каждую страницу в указанном срезе
        for page_num, page_text_obj in enumerate(pages_slice, start=start_page):
            # Используем pdfminer для более подробной обработки страницы
            for page in extract_pages(self.pdf_file, page_numbers=[page_num]):
                page_content = self.process_page(page_num, page)
                self.text_per_page[page_num] = page_content

    def process_page(self, pagenum: int, page: LTPage) -> Dict[str, Any]:
        """
        Processes a single page of the PDF, extracting text, tables, and formatting.

        Args:
            pagenum (int): The page number.
            page (LTPage): The page object from pdfminer.

        Returns:
            Dict[str, Any]: A dictionary with extracted text, formatting, and other page content.
        """
        page_text: List[str] = []
        line_format: List[str] = []
        text_from_images: List[str] = []
        text_from_tables: List[str] = []
        page_content: List[str] = []

        table_num = 0
        first_element = True
        table_extraction_flag = False

        # Используем pdfplumber для извлечения таблиц на указанной странице
        with pdfplumber.open(self.pdf_file) as pdf:
            page_tables = pdf.pages[pagenum]
            tables = page_tables.find_tables()

        page_elements = [(element.y1, element) for element in page._objs]
        page_elements.sort(key=lambda a: a[0], reverse=True)
        for i, component in enumerate(page_elements):
            pos = component[0]
            element = component[1]

            if isinstance(element, LTTextContainer):
                if not table_extraction_flag:
                    line_text, format_per_line = text_extraction(element)
                    page_text.append(line_text)
                    line_format.append(format_per_line)
                    page_content.append(line_text)

            if isinstance(element, LTRect):
                if first_element and (table_num + 1) <= len(tables):
                    lower_side = page.bbox[3] - tables[table_num].bbox[3]
                    upper_side = element.y1
                    table = extract_table(self.pdf_file, pagenum, table_num)
                    table_string = table_converter(table)
                    text_from_tables.append(table_string)
                    page_content.append(table_string)
                    table_extraction_flag = True
                    first_element = False
                    page_text.append('table')
                    line_format.append('table')

                    if element.y0 >= lower_side and element.y1 <= upper_side:
                        pass
                elif i + 1 < len(page_elements) and not isinstance(page_elements[i + 1][1], LTRect):
                    # Проверка на выход за пределы списка
                    table_extraction_flag = False
                    first_element = True
                    table_num += 1

        return {
            'text': page_text,
            'line_format': line_format,
            'text_from_images': text_from_images,
            'text_from_tables': text_from_tables,
            'page_content': page_content,
        }

    def extract(self):
        """
        Generator function that yields the extracted content for each page in the PDF, including text and tables.

        Yields:
            str: A formatted string for each page that includes the page key, text from tables,
                 and main text content, separated by spaces and followed by newlines for readability.

        Example:
            For each page in the PDF, this function will yield a formatted string in the following format:
                'Page_X <table_text_1> <table_text_2> ... <text_line_1> <text_line_2> ...'
        """
        for key, value in self.text_per_page.items():
            yield ' '.join(value['text_from_tables'] + value['text'])


"""
--- EXAMPLE USAGE ---
"""
### pip install pdfplumber PyPDF2 pdfminer.six - эти либы поставь

from pathlib import Path

pdf_path = Path("files/Хобл.pdf") # тут путь к файлу

with pdf_path.open("rb") as f:
    processor = PDFProcessor(f)
    processor.process_pdf()

    for page_num, page_content in enumerate(processor.extract(), start=1):
        print(f"\n=== Страница {page_num} ===")
        print(page_content)

================================================================================
ФАЙЛ: package/pdf/tools/__init__.py
================================================================================

from .formatter import extract_table, table_converter, text_extraction

================================================================================
ФАЙЛ: package/pdf/tools/formatter.py
================================================================================

from typing import List, Optional

import pdfplumber
from pdfminer.layout import (
    LTChar,
    LTTextBoxHorizontal,
    LTTextContainer,
)

TableType = List[List[Optional[str]]]


def text_extraction(element: LTTextBoxHorizontal) -> (str, list):
    """
    Function for extraction text from pdf page.
    Args:
        element: TextBox element from pdfminer library.

    Returns: Two value. Text in current page and all format text in page.
    """
    line_text = element.get_text()
    line_formats = []
    for text_line in element:
        if not isinstance(text_line, LTTextContainer):
            continue
        for character in text_line:
            if not isinstance(character, LTChar):
                continue
            line_formats.append(character.fontname)
            line_formats.append(character.size)
            line_formats.append(f'upright - {character.upright}')
            line_formats.append(f'adv - {character.adv}')

    return line_text, list(set(line_formats))


def extract_table(pdf_path: str, page_num: int, table_num: int) -> TableType:
    """
    Function for get text from tables from pdf document.
    Args:
        pdf_path: path to pdf file.
        page_num: num pdf page.
        table_num: num table in pdf document.

    Returns: Data of tables.

    """
    pdf = pdfplumber.open(pdf_path)
    table_page = pdf.pages[page_num]
    table = table_page.extract_tables()[table_num]
    return table


def table_converter(table: TableType) -> str:
    """
    Function for convert table view to text.
    Args:
        table: Table object (pandas table object)

    Returns: String view of table in pdf file.

    """
    table_string = ''
    for row_num in range(len(table)):
        row = table[row_num]
        cleaned_row = [
            item.replace('\n', ' ') if item is not None and '\n' in item else 'None' if item is None else item for item
            in row]
        table_string += ('|' + '|'.join(cleaned_row) + '|' + '\n')
    table_string = table_string[:-1]
    return table_string

================================================================================
ФАЙЛ: requirements.txt
================================================================================

PyPDF2>=3.0.0
pdfminer.six>=20221105
pdfplumber>=0.9.0
aiohttp>=3.8.0
pydantic>=1.10.0
pydantic-settings>=1.3.0
openai>=0.27.8
pytesseract>=0.3.10
tiktoken>=0.5.0
langchain-openai>=0.2.0
markdown2>=2.4.10
pdfkit>=1.0.0
markdown-pdf>=0.11.0
minio>=7.1.8
pymilvus>=1.2.3
sqlalchemy>=2.0.21
fastapi>=0.103.0
alembic>=1.12.0
asyncpg>=0.29.0
greenlet>=2.1.4
uvicorn==0.32.1
python-multipart
celery==5.2.7
flower==1.2.0
redis
langchain
gunicorn
aiogram==3.17.0
flask==3.1.0

================================================================================
ФАЙЛ: setup.cfg
================================================================================

[isort]
# Isort configuration:
line_length = 70
multi_line_output = 3
use_parentheses = true
include_trailing_comma = true
extend_skip = migrations/versions


[flake8]
# Base flake8 configuration:
format = wemake
doctests = true
show-source = true
statistics = false

# Plugins:
max-methods = 9
max-arguments = 10
min-name-length = 1
max-expressions = 15
max-line-length = 125
max-string-usages = 10
max-line-complexity = 20
max-attributes = 20

# Self settings:
i-control-code = false
extend-immutable-calls = Depends
nested-classes-whitelist = Meta, Params, Config

# Excluding some directories:
exclude =
    .git
    .venv
    venv
    .idea
    .vscode
    .mypy_cache
    __pycache__
    migrations
    main.py
    package/pdf/tools/formatter.py
    package/openai/prompts/*

# Violations:
ignore = D100, D101, D102, D103, D104, D105, D106, D107, N805, WPS114, WPS332, WPS354, WPS404, WPS432, WPS305, WPS462, WPS318
per-file-ignores =
    response*: WPS114
    settings*: WPS115
    __init__.py: WPS300, F401, F403, WPS347
    package/openai/__init__.py: WPS300, F401, F403, WPS347
    package/openai/prompts/__init__.py: WPS300, F401, F403, WPS347
    package/milvus/__init__.py: WPS300, F401, F403, WPS347
    service.py: WPS110, WPS125
    package/rabbitmq/rpc/routing.py: S101
    internal/dto/rule/filter/*: WPS102, WPS110, WPS201, WPS300, F401
    package/openai/*: D205, W391, RST301, RST201, D202

